{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GeneralTools.misc_fun import FLAGS\n",
    "\n",
    "FLAGS.TENSORFLOW_VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset provided by:\n",
    "Mohan S Acharya, Asfia Armaan, Aneeta S Antony. \n",
    "A Comparison of Regression Models for Prediction of Graduate Admissions, \n",
    "IEEE International Conference on Computational Intelligence in Data Science 2019.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# csv file can also be read using pandas\n",
    "data_file = np.genfromtxt(\n",
    "    os.path.join(FLAGS.DEFAULT_IN, 'graduate-admissions/Admission_Predict_Ver1.1.csv'), \n",
    "    delimiter=',', skip_header=1)  # the first row is header, which we skip\n",
    "data_file.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[337.   118.     4.     4.5    4.5    9.65   1.  ]\n",
      " [324.   107.     4.     4.     4.5    8.87   1.  ]\n",
      " [316.   104.     3.     3.     3.5    8.     1.  ]\n",
      " [322.   110.     3.     3.5    2.5    8.67   1.  ]]\n",
      "[0.92 0.76 0.72 0.8 ]\n",
      "(500,)\n",
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "# The first row is header, the first column is index, the last column is label\n",
    "x_np = data_file[:, 1:8]\n",
    "y_np = data_file[:, 8]\n",
    "print(x_np[0:4])\n",
    "print(y_np[0:4])\n",
    "print(y_np.shape)\n",
    "y_np = np.expand_dims(y_np, axis=1)\n",
    "print(y_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Engi/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/Engi/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "The decay step is 9\n",
      "Step 1: training loss is 3.6521; test loss is 3.4963\n",
      "Step 51: training loss is 0.2078; test loss is 0.2091\n",
      "Step 101: training loss is 0.1833; test loss is 0.1402\n",
      "Step 151: training loss is 0.1686; test loss is 0.1572\n",
      "Step 201: training loss is 0.1811; test loss is 0.2090\n",
      "Step 251: training loss is 0.1735; test loss is 0.1445\n",
      "Step 301: training loss is 0.1838; test loss is 0.1155\n",
      "Step 351: training loss is 0.1739; test loss is 0.2458\n",
      "Step 401: training loss is 0.1739; test loss is 0.2290\n",
      "Step 451: training loss is 0.1852; test loss is 0.2359\n",
      "Step 501: training loss is 0.1641; test loss is 0.2001\n",
      "Step 551: training loss is 0.1834; test loss is 0.2159\n",
      "Step 601: training loss is 0.1761; test loss is 0.1827\n",
      "Step 651: training loss is 0.1809; test loss is 0.2319\n",
      "Step 701: training loss is 0.1828; test loss is 0.2153\n",
      "Step 751: training loss is 0.1753; test loss is 0.1457\n",
      "Step 801: training loss is 0.1729; test loss is 0.1774\n",
      "Step 851: training loss is 0.1827; test loss is 0.2214\n",
      "Step 901: training loss is 0.1777; test loss is 0.1774\n",
      "Step 951: training loss is 0.1518; test loss is 0.2101\n",
      "Step 1000: training loss is 0.1774; test loss is 0.1167\n",
      "Training for 1000 steps took 3.114 sec.\n"
     ]
    }
   ],
   "source": [
    "# simple linear regression\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "tf.Graph() initializes a new graph.\n",
    "some_graph.as_default() sets the some_graph as the graph to work on.\n",
    "with some_graph defines a scope. All TensorFlow operations should be under the same graph/scope. \n",
    "\"\"\"\n",
    "with tf.Graph().as_default(), tf.device('cpu:0'):\n",
    "    # Workflow step 1. prepare data\n",
    "    # tf.constant is one way to add the data to graph\n",
    "    data = tf.constant(data_file[:, 1:], dtype=tf.float32)\n",
    "    # shuffle the dataset\n",
    "    data_shuffle = tf.random.shuffle(data, name='data_shuffle')\n",
    "    # divide the dataset into training and test datasets\n",
    "    data_tr = data_shuffle[0:400]  # 400-by-8, print(data_tr.get_shape().as_list()) -> [400, 8]\n",
    "    data_te = data_shuffle[400:500]  # 100-by-8\n",
    "    # scale the data\n",
    "    mean_tr, var_tr = tf.nn.moments(data_tr, axes=[0], keep_dims=True)\n",
    "    std_tr = tf.sqrt(var_tr + FLAGS.EPSI)  # print(mean_tr.get_shape().as_list()) -> [1, 8]\n",
    "    data_tr = (data_tr - mean_tr) / std_tr\n",
    "    data_te = (data_te - mean_tr) / std_tr\n",
    "    # separate the features from targets\n",
    "    x_tr = data_tr[:, :7]\n",
    "    t_tr = tf.expand_dims(data_tr[:, 7], axis=1)\n",
    "    x_te = data_te[:, :7]\n",
    "    t_te = tf.expand_dims(data_te[:, 7], axis=1)\n",
    "    \n",
    "    # Workflow step 2. define the model\n",
    "    \"\"\"\n",
    "    There are at least three ways to initialize trainable parameters:\n",
    "    Using low level API:\n",
    "        1. tf.Variable always initializes a new tensor when called;\n",
    "        2. tf.get_variable initializes a new tensor when first called, can recall a previously \n",
    "        initialized tensor when called again (if variable scope is set properly);\n",
    "    Using high level API\n",
    "        3. tf.layer.* implicitly initializes trainable parameters using tf.get_variable.\n",
    "    \n",
    "    when trainable=True, the parameters are added to GraphKeys.TRAINABLE_VARIABLES and can be \n",
    "    returned using tf.trainable_variables() or tf.get_collection().\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('LinearModel0', reuse=tf.AUTO_REUSE):\n",
    "        w0 = tf.get_variable(\n",
    "            'weight', shape=(x_np.shape[1], y_np.shape[1]), dtype=tf.float32, \n",
    "            initializer=tf.random_normal_initializer, trainable=True)\n",
    "        b0 = tf.get_variable(  # optional if we standardize the inputs and outputs in a linear model\n",
    "            'bias', shape=[], dtype=tf.float32, \n",
    "            initializer=tf.random_normal_initializer, trainable=True)\n",
    "        # print(tf.trainable_variables()) -> [<tf.Variable 'weight:0' shape=(7, 1) dtype=float32_ref>]\n",
    "        y_tr = tf.matmul(x_tr, w0) + b0\n",
    "        y_te = tf.matmul(x_te, w0) + b0\n",
    "    with tf.name_scope('LinearModel1'):\n",
    "        pass\n",
    "    \n",
    "    # Workflow step 3. define the loss function\n",
    "    \"\"\"\n",
    "    tf.losses contains several standard loss functions like MSE, MAE, cross-entropy, hinge\n",
    "    User-defined loss can be added.\n",
    "    The loss function constructed will by default be added to tf.GraphKeys.LOSSES\n",
    "    https://www.tensorflow.org/api_docs/python/tf/losses\n",
    "    \"\"\"\n",
    "    loss = tf.losses.mean_squared_error(\n",
    "        t_tr, y_tr, reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE, scope='train_loss')\n",
    "    loss_te = tf.losses.mean_squared_error(\n",
    "        t_te, y_te, reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE, scope='test_loss')\n",
    "    \n",
    "    # Workflow step 4. configure the optimizer\n",
    "    # configure learning rate\n",
    "    global_step = tf.get_variable(\n",
    "        name='global_step', shape=[], dtype=tf.int32, \n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "    # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    init_learning_rate = 0.1\n",
    "    decay_steps = np.round(1000 * np.log(0.96) / np.log(0.001 / init_learning_rate)).astype(np.int32)\n",
    "    print('The decay step is {}'.format(decay_steps))\n",
    "    learning_rate = tf.train.exponential_decay(  # adaptive learning rate\n",
    "            0.5,\n",
    "            global_step=global_step,\n",
    "            decay_steps=decay_steps,\n",
    "            decay_rate=0.96,\n",
    "            staircase=False, name='decay_lr')\n",
    "    # configure optimizer\n",
    "    # see other optimizers at: https://www.tensorflow.org/api_docs/python/tf/train\n",
    "    # include: MomentumOptimizer, AdamOptimizer, RMSPropOptimizer\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=init_learning_rate, name='GD')\n",
    "    # configure optimization process\n",
    "    # the following can also be done as opt.minimize(loss, global_step=global_step)\n",
    "    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=None)\n",
    "    grads_list = opt.compute_gradients(loss, var_list)\n",
    "    opt_op = opt.apply_gradients(grads_list, global_step=global_step)\n",
    "    \n",
    "    # Workflow step 5. configure the summary\n",
    "    # summary op is always pinned to CPU\n",
    "    tf.summary.histogram('weights', w0)\n",
    "    tf.summary.scalar('bias', b0)  # normally bias would be a vector, so histogram too\n",
    "    tf.summary.scalar('loss/train', loss)\n",
    "    tf.summary.scalar('loss/test', loss_te)\n",
    "    for grads in grads_list:\n",
    "        var_grad = grads[0]\n",
    "        var = grads[1]\n",
    "        var_name = var.name.replace(':', '_')\n",
    "        tf.summary.histogram('grad_' + var_name, var_grad)\n",
    "        tf.summary.histogram(var_name, var)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_folder = os.path.join(FLAGS.DEFAULT_OUT, 'graduate_admission' + '_log')\n",
    "    if not os.path.exists(summary_folder):\n",
    "        os.makedirs(summary_folder)\n",
    "    \n",
    "    # Workflow step 6. call a session\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False))\n",
    "    # initialize the graph\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # configure where to write the summary file\n",
    "    summary_writer = tf.summary.FileWriter(summary_folder, sess.graph)\n",
    "    # run the session\n",
    "    start_time = time.time()\n",
    "    max_iter = 1000\n",
    "    for step in range(max_iter):\n",
    "        loss_value, _, global_step_value = sess.run([loss, opt_op, global_step])\n",
    "        # check if model produces nan outcome\n",
    "        assert not np.isnan(loss_value), \\\n",
    "            'Model diverged with loss = {} at step {}'.format(loss_value, global_step_value)\n",
    "        \n",
    "        # add summary and print loss every query step\n",
    "        if global_step_value % 50 == 1 or global_step_value == max_iter:\n",
    "            loss_te_value = sess.run(loss_te)\n",
    "            if summary_op is not None:\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, global_step=global_step_value)\n",
    "            print('Step {}: training loss is {:.4f}; test loss is {:.4f}'.format(\n",
    "                global_step_value, loss_value, loss_te_value))\n",
    "            \n",
    "    duration = time.time() - start_time\n",
    "    print('Training for {} steps took {:.3f} sec.'.format(max_iter, duration))\n",
    "    sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
